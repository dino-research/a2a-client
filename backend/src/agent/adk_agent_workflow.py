"""
ADK-based research agent using Workflow Agents pattern.
Implements specialized LlmAgents for each research task and orchestrates them using Sequential/Loop Agents.
Updated to follow ADK v1.0.0 best practices with Workflow Agents.
"""
import os
import json
from datetime import datetime
from typing import Dict, List, Optional
from google.adk.agents import LlmAgent, SequentialAgent, LoopAgent
from tavily import TavilyClient

# Load environment variables
from dotenv import load_dotenv
load_dotenv()

# Import prompts
from prompts import (
    get_web_research_prompt,
    get_synthesis_prompt,
    get_research_agent_instruction
)

# Global variable to store current effort settings
_current_effort_settings = {
    "initial_search_query_count": 3,
    "max_research_loops": 3
}

def set_effort_settings(initial_search_query_count: int, max_research_loops: int):
    """Set global effort settings for this research session."""
    global _current_effort_settings
    _current_effort_settings = {
        "initial_search_query_count": initial_search_query_count,
        "max_research_loops": max_research_loops
    }

def get_current_date() -> str:
    """Get current date in a readable format."""
    return datetime.now().strftime("%B %d, %Y")

# ============================================================================
# COORDINATOR AGENT FOR DETERMINING RESEARCH APPROACH
# ============================================================================

def create_coordinator_agent(model: str = "gemini-2.0-flash") -> LlmAgent:
    """
    Create an LlmAgent that coordinates and decides whether web research is needed.
    """
    
    def tavily_research_tool(query: str) -> Dict:
        """Tool function for Tavily research"""
        try:
            tavily_api_key = os.getenv("TAVILY_API_KEY")
            if not tavily_api_key:
                return {
                    "status": "error",
                    "query": query,
                    "error": "TAVILY_API_KEY not found in environment variables",
                    "research_date": get_current_date()
                }
            
            tavily_client = TavilyClient(api_key=tavily_api_key)
            current_date = get_current_date()
            
            # Perform search with Tavily
            search_result = tavily_client.search(
                query=query,
                search_depth="advanced",
                max_results=5,
                include_answer=True,
                include_raw_content=False,
                include_domains=None,
                exclude_domains=None
            )
            
            # Extract sources from Tavily results
            sources = []
            search_content = ""
            
            if search_result.get("answer"):
                search_content = search_result["answer"]
            
            # Process search results
            if search_result.get("results"):
                for result in search_result["results"]:
                    sources.append({
                        "title": result.get("title", "KhÃ´ng cÃ³ tiÃªu Ä‘á»"),
                        "url": result.get("url", ""),
                        "snippet": result.get("content", "")[:300] + "..." if result.get("content") else ""
                    })
                    
                    # Append content for comprehensive research
                    if result.get("content"):
                        search_content += f"\n\n{result['content'][:500]}..."
            
            # If no answer was provided by Tavily, create summary from results
            if not search_content and sources:
                search_content = f"Káº¿t quáº£ tÃ¬m kiáº¿m cho '{query}':\n\n"
                for i, source in enumerate(sources[:3], 1):
                    search_content += f"{i}. {source['title']}: {source['snippet']}\n\n"
            
            return {
                "status": "success",
                "query": query,
                "content": search_content,
                "sources": sources,
                "research_date": current_date,
                "search_engine": "Tavily"
            }
            
        except Exception as e:
            return {
                "status": "error",
                "query": query,
                "error": str(e),
                "research_date": get_current_date()
            }
    
    instruction = f"""
Báº¡n lÃ  má»™t AI assistant thÃ´ng minh cÃ³ kháº£ nÄƒng phÃ¢n tÃ­ch cÃ¢u há»i vÃ  quyáº¿t Ä‘á»‹nh cÃ¡ch tráº£ lá»i tá»‘i Æ°u. NgÃ y hiá»‡n táº¡i: {get_current_date()}

**QUY TRÃŒNH Xá»¬ LÃ:**

BÆ¯á»šC 1: PhÃ¢n tÃ­ch cÃ¢u há»i
- CÃ¢u há»i cÃ¡ nhÃ¢n/chÃ o há»i/giá»›i thiá»‡u â†’ Tráº£ lá»i trá»±c tiáº¿p
- Kiáº¿n thá»©c cÆ¡ báº£n khÃ´ng Ä‘á»•i theo thá»i gian â†’ Tráº£ lá»i trá»±c tiáº¿p  
- ThÃ´ng tin cáº§n cáº­p nháº­t/tin tá»©c/dá»¯ liá»‡u thá»i gian thá»±c â†’ TÃ¬m kiáº¿m web

BÆ¯á»šC 2: Thá»±c hiá»‡n
- Náº¿u tráº£ lá»i trá»±c tiáº¿p: ÄÆ°a ra cÃ¢u tráº£ lá»i hoÃ n chá»‰nh báº±ng tiáº¿ng Viá»‡t
- Náº¿u cáº§n tÃ¬m kiáº¿m: Sá»­ dá»¥ng tavily_research_tool rá»“i tá»•ng há»£p káº¿t quáº£

**VÃ Dá»¤:**

CÃ¢u há»i: "ChÃ o báº¡n, tÃ´i lÃ  ThÃ¡i"
â†’ Tráº£ lá»i trá»±c tiáº¿p: "ChÃ o báº¡n ThÃ¡i! TÃ´i lÃ  AI assistant, ráº¥t vui Ä‘Æ°á»£c lÃ m quen vá»›i báº¡n. TÃ´i cÃ³ thá»ƒ giÃºp báº¡n tráº£ lá»i cÃ¢u há»i, tÃ¬m kiáº¿m thÃ´ng tin hoáº·c há»— trá»£ trong nhiá»u cÃ´ng viá»‡c khÃ¡c. Báº¡n cáº§n tÃ´i giÃºp gÃ¬ khÃ´ng?"

CÃ¢u há»i: "Thá»i tiáº¿t HÃ  Ná»™i hÃ´m nay tháº¿ nÃ o?"
â†’ TÃ¬m kiáº¿m web vá»›i query "thá»i tiáº¿t HÃ  Ná»™i hÃ´m nay" rá»“i tá»•ng há»£p káº¿t quáº£

CÃ¢u há»i: "2 + 2 báº±ng bao nhiÃªu?"
â†’ Tráº£ lá»i trá»±c tiáº¿p: "2 + 2 = 4"

**LÆ¯U Ã:**
- LuÃ´n tráº£ lá»i báº±ng tiáº¿ng Viá»‡t
- KhÃ´ng tráº£ vá» JSON, chá»‰ tráº£ lá»i trá»±c tiáº¿p hoáº·c sá»­ dá»¥ng tool
- Khi sá»­ dá»¥ng tool, hÃ£y tá»•ng há»£p káº¿t quáº£ thÃ nh cÃ¢u tráº£ lá»i hoÃ n chá»‰nh
"""
    
    return LlmAgent(
        name="coordinator",
        model=model,
        tools=[tavily_research_tool],
        instruction=instruction,
        description="Agent Ä‘iá»u phá»‘i thÃ´ng minh cÃ³ kháº£ nÄƒng tráº£ lá»i trá»±c tiáº¿p hoáº·c tÃ¬m kiáº¿m web"
    )

# ============================================================================
# SPECIALIZED LLM AGENTS FOR EACH RESEARCH TASK
# ============================================================================

def create_query_generator_agent(model: str = "gemini-2.0-flash") -> LlmAgent:
    """
    Create an LlmAgent specialized in generating search queries.
    """
    instruction = f"""
Báº¡n lÃ  chuyÃªn gia táº¡o query tÃ¬m kiáº¿m. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  táº¡o ra {_current_effort_settings['initial_search_query_count']} query tÃ¬m kiáº¿m hiá»‡u quáº£ tá»« cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng.

NguyÃªn táº¯c:
1. Táº¡o query Ä‘a dáº¡ng Ä‘á»ƒ bao phá»§ nhiá»u gÃ³c Ä‘á»™
2. Bao gá»“m tá»« khÃ³a chÃ­nh vÃ  tá»« khÃ³a phá»¥
3. Xem xÃ©t cáº£ tiáº¿ng Viá»‡t vÃ  tiáº¿ng Anh náº¿u cáº§n
4. Tá»‘i Æ°u cho search engine

Äá»‹nh dáº¡ng output: Tráº£ vá» list cÃ¡c query dÆ°á»›i dáº¡ng JSON array
VÃ­ dá»¥: ["query 1", "query 2", "query 3"]

NgÃ y hiá»‡n táº¡i: {get_current_date()}
"""
    
    return LlmAgent(
        name="query_generator",
        model=model,
        instruction=instruction,
        description="Specialized agent for generating effective search queries"
    )

def create_web_research_agent(model: str = "gemini-2.0-flash") -> LlmAgent:
    """
    Create an LlmAgent specialized in web research using Tavily.
    """
    
    def tavily_research_tool(query: str) -> Dict:
        """Tool function for Tavily research"""
        try:
            tavily_api_key = os.getenv("TAVILY_API_KEY")
            if not tavily_api_key:
                return {
                    "status": "error",
                    "query": query,
                    "error": "TAVILY_API_KEY not found in environment variables",
                    "research_date": get_current_date()
                }
            
            tavily_client = TavilyClient(api_key=tavily_api_key)
            current_date = get_current_date()
            
            # Perform search with Tavily
            search_result = tavily_client.search(
                query=query,
                search_depth="advanced",
                max_results=5,
                include_answer=True,
                include_raw_content=False,
                include_domains=None,
                exclude_domains=None
            )
            
            # Extract sources from Tavily results
            sources = []
            search_content = ""
            
            if search_result.get("answer"):
                search_content = search_result["answer"]
            
            # Process search results
            if search_result.get("results"):
                for result in search_result["results"]:
                    sources.append({
                        "title": result.get("title", "KhÃ´ng cÃ³ tiÃªu Ä‘á»"),
                        "url": result.get("url", ""),
                        "snippet": result.get("content", "")[:300] + "..." if result.get("content") else ""
                    })
                    
                    # Append content for comprehensive research
                    if result.get("content"):
                        search_content += f"\n\n{result['content'][:500]}..."
            
            # If no answer was provided by Tavily, create summary from results
            if not search_content and sources:
                search_content = f"Káº¿t quáº£ tÃ¬m kiáº¿m cho '{query}':\n\n"
                for i, source in enumerate(sources[:3], 1):
                    search_content += f"{i}. {source['title']}: {source['snippet']}\n\n"
            
            return {
                "status": "success",
                "query": query,
                "content": search_content,
                "sources": sources,
                "research_date": current_date,
                "search_engine": "Tavily"
            }
            
        except Exception as e:
            return {
                "status": "error",
                "query": query,
                "error": str(e),
                "research_date": get_current_date()
            }
    
    instruction = """
Báº¡n lÃ  chuyÃªn gia nghiÃªn cá»©u web. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  thá»±c hiá»‡n tÃ¬m kiáº¿m web vá»›i cÃ¡c query Ä‘Æ°á»£c cung cáº¥p.

NguyÃªn táº¯c:
1. Thá»±c hiá»‡n tÃ¬m kiáº¿m vá»›i tá»«ng query má»™t cÃ¡ch ká»¹ lÆ°á»¡ng
2. Thu tháº­p thÃ´ng tin tá»« nhiá»u nguá»“n Ä‘Ã¡ng tin cáº­y
3. Tá»•ng há»£p vÃ  phÃ¢n tÃ­ch thÃ´ng tin thu Ä‘Æ°á»£c
4. ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng vÃ  Ä‘á»™ tin cáº­y cá»§a nguá»“n

Äá»‹nh dáº¡ng output: Tráº£ vá» káº¿t quáº£ nghiÃªn cá»©u dÆ°á»›i dáº¡ng JSON vá»›i cÃ¡c trÆ°á»ng:
- status: "success" hoáº·c "error"
- content: Ná»™i dung nghiÃªn cá»©u
- sources: Danh sÃ¡ch cÃ¡c nguá»“n
- summary: TÃ³m táº¯t thÃ´ng tin chÃ­nh

Sá»­ dá»¥ng tool tavily_research_tool Ä‘á»ƒ thá»±c hiá»‡n tÃ¬m kiáº¿m.
"""
    
    return LlmAgent(
        name="web_researcher",
        model=model,
        tools=[tavily_research_tool],
        instruction=instruction,
        description="Specialized agent for conducting web research"
    )

def create_quality_analyzer_agent(model: str = "gemini-2.0-flash") -> LlmAgent:
    """
    Create an LlmAgent specialized in analyzing research quality.
    """
    instruction = """
Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch cháº¥t lÆ°á»£ng nghiÃªn cá»©u. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng vÃ  tÃ­nh Ä‘áº§y Ä‘á»§ cá»§a káº¿t quáº£ nghiÃªn cá»©u.

TiÃªu chÃ­ Ä‘Ã¡nh giÃ¡:
1. Sá»‘ lÆ°á»£ng nguá»“n tin Ä‘Ã¡ng tin cáº­y
2. TÃ­nh Ä‘a dáº¡ng cá»§a thÃ´ng tin
3. Äá»™ má»›i cá»§a thÃ´ng tin
4. TÃ­nh liÃªn quan Ä‘áº¿n cÃ¢u há»i gá»‘c
5. TÃ­nh Ä‘áº§y Ä‘á»§ Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i

Thang Ä‘iá»ƒm:
- 0.8-1.0: Cháº¥t lÆ°á»£ng cao, Ä‘á»§ thÃ´ng tin
- 0.6-0.7: Cháº¥t lÆ°á»£ng trung bÃ¬nh, cáº§n thÃªm thÃ´ng tin
- 0.0-0.5: Cháº¥t lÆ°á»£ng tháº¥p, cáº§n nghiÃªn cá»©u thÃªm nhiá»u

Äá»‹nh dáº¡ng output: JSON vá»›i cÃ¡c trÆ°á»ng:
- confidence: Äiá»ƒm tin cáº­y (0.0-1.0)
- is_sufficient: true/false
- recommendation: "finalize_answer", "additional_research", hoáº·c "need_more_research"
- gaps_identified: Danh sÃ¡ch cÃ¡c khoáº£ng trá»‘ng thÃ´ng tin cáº§n bá»• sung
- reason: LÃ½ do Ä‘Æ°a ra Ä‘Ã¡nh giÃ¡ nÃ y
"""
    
    return LlmAgent(
        name="quality_analyzer",
        model=model,
        instruction=instruction,
        description="Specialized agent for analyzing research quality and completeness"
    )

def create_refinement_agent(model: str = "gemini-2.0-flash") -> LlmAgent:
    """
    Create an LlmAgent specialized in generating refinement queries.
    """
    max_queries = min(_current_effort_settings["initial_search_query_count"], 3)
    
    instruction = f"""
Báº¡n lÃ  chuyÃªn gia táº¡o query bá»• sung. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  táº¡o ra tá»‘i Ä‘a {max_queries} query bá»• sung Ä‘á»ƒ láº¥p Ä‘áº§y khoáº£ng trá»‘ng thÃ´ng tin.

NguyÃªn táº¯c:
1. PhÃ¢n tÃ­ch khoáº£ng trá»‘ng thÃ´ng tin tá»« Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng
2. Táº¡o query cá»¥ thá»ƒ Ä‘á»ƒ tÃ¬m thÃ´ng tin cÃ²n thiáº¿u
3. Táº­p trung vÃ o cÃ¡c khÃ­a cáº¡nh chÆ°a Ä‘Æ°á»£c khÃ¡m phÃ¡
4. Æ¯u tiÃªn thÃ´ng tin má»›i nháº¥t vÃ  Ä‘Ã¡ng tin cáº­y

Äá»‹nh dáº¡ng output: JSON array cÃ¡c query bá»• sung
VÃ­ dá»¥: ["query bá»• sung 1", "query bá»• sung 2"]

Náº¿u khÃ´ng cáº§n thÃªm nghiÃªn cá»©u, tráº£ vá» array rá»—ng: []
"""
    
    return LlmAgent(
        name="refinement_specialist",
        model=model,
        instruction=instruction,
        description="Specialized agent for generating follow-up research queries"
    )

def create_answer_finalizer_agent(model: str = "gemini-2.0-flash") -> LlmAgent:
    """
    Create an LlmAgent specialized in synthesizing final answers.
    """
    instruction = """
Báº¡n lÃ  chuyÃªn gia tá»•ng há»£p thÃ´ng tin vÃ  viáº¿t cÃ¢u tráº£ lá»i cuá»‘i cÃ¹ng. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  táº¡o ra cÃ¢u tráº£ lá»i toÃ n diá»‡n, chÃ­nh xÃ¡c vÃ  cÃ³ trÃ­ch dáº«n nguá»“n.

NguyÃªn táº¯c:
1. Tá»•ng há»£p táº¥t cáº£ thÃ´ng tin tá»« cÃ¡c nguá»“n Ä‘Ã£ nghiÃªn cá»©u
2. Tráº£ lá»i trá»±c tiáº¿p cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng
3. Sá»­ dá»¥ng ngÃ´n ngá»¯ rÃµ rÃ ng, dá»… hiá»ƒu
4. Bao gá»“m trÃ­ch dáº«n nguá»“n Ä‘Ã¡ng tin cáº­y
5. Cáº¥u trÃºc cÃ¢u tráº£ lá»i logic vÃ  máº¡ch láº¡c

Äá»‹nh dáº¡ng output:
- CÃ¢u tráº£ lá»i chÃ­nh (báº±ng tiáº¿ng Viá»‡t)
- Pháº§n nguá»“n thÃ´ng tin vá»›i format markdown links

VÃ­ dá»¥:
CÃ¢u tráº£ lá»i vá» chá»§ Ä‘á»...

**Nguá»“n thÃ´ng tin:**
1. [TiÃªu Ä‘á» nguá»“n 1](URL)
2. [TiÃªu Ä‘á» nguá»“n 2](URL)
"""
    
    return LlmAgent(
        name="answer_finalizer",
        model=model,
        instruction=instruction,
        description="Specialized agent for synthesizing comprehensive final answers"
    )

# ============================================================================
# WORKFLOW ORCHESTRATION
# ============================================================================

def create_research_workflow_agent(model: str = "gemini-2.0-flash") -> SequentialAgent:
    """
    Create a SequentialAgent that orchestrates the research workflow.
    """
    # Create specialized agents
    query_generator = create_query_generator_agent(model)
    web_researcher = create_web_research_agent(model)
    quality_analyzer = create_quality_analyzer_agent(model)
    answer_finalizer = create_answer_finalizer_agent(model)
    
    # Create sequential workflow
    research_workflow = SequentialAgent(
        name="research_workflow",
        sub_agents=[
            query_generator,
            web_researcher,
            quality_analyzer,
            answer_finalizer
        ],
        description="Sequential workflow for comprehensive research and answer generation"
    )
    
    return research_workflow

def create_iterative_research_agent(model: str = "gemini-2.0-flash") -> LoopAgent:
    """
    Create a LoopAgent for iterative research with quality checking.
    """
    max_loops = _current_effort_settings["max_research_loops"]
    
    # Create the research loop components
    query_generator = create_query_generator_agent(model)
    web_researcher = create_web_research_agent(model)
    quality_analyzer = create_quality_analyzer_agent(model)
    refinement_agent = create_refinement_agent(model)
    
    # Create a sequential sub-workflow for each iteration
    iteration_workflow = SequentialAgent(
        name="research_iteration",
        sub_agents=[
            query_generator,
            web_researcher,
            quality_analyzer,
            refinement_agent
        ],
        description="Single iteration of research, analysis, and refinement"
    )
    
    # Create loop agent (simplified without complex termination condition)
    iterative_researcher = LoopAgent(
        name="iterative_researcher",
        sub_agents=[iteration_workflow],
        max_iterations=max_loops,
        description=f"Iterative research loop with max {max_loops} iterations"
    )
    
    return iterative_researcher

# ============================================================================
# MAIN RESEARCH AGENT FACTORY
# ============================================================================

def create_simple_answer_agent(model: str = "gemini-2.0-flash") -> LlmAgent:
    """
    Create an LlmAgent for answering simple questions directly without web research.
    """
    instruction = """
Báº¡n lÃ  má»™t AI assistant thÃ´ng minh, cÃ³ thá»ƒ tráº£ lá»i cÃ¡c cÃ¢u há»i cÆ¡ báº£n mÃ  khÃ´ng cáº§n tÃ¬m kiáº¿m thÃ´ng tin trÃªn web.

Nhiá»‡m vá»¥ cá»§a báº¡n:
1. Tráº£ lá»i cÃ¡c cÃ¢u há»i cÃ¡ nhÃ¢n, chÃ o há»i, giá»›i thiá»‡u
2. Giáº£i Ä‘Ã¡p cÃ¡c cÃ¢u há»i kiáº¿n thá»©c cÆ¡ báº£n
3. Thá»±c hiá»‡n cÃ¡c phÃ©p toÃ¡n Ä‘Æ¡n giáº£n
4. Cung cáº¥p Ä‘á»‹nh nghÄ©a vÃ  giáº£i thÃ­ch khÃ¡i niá»‡m cÆ¡ báº£n

NguyÃªn táº¯c:
- Tráº£ lá»i má»™t cÃ¡ch thÃ¢n thiá»‡n vÃ  há»¯u Ã­ch
- Sá»­ dá»¥ng ngÃ´n ngá»¯ rÃµ rÃ ng, dá»… hiá»ƒu
- Thá»«a nháº­n náº¿u khÃ´ng cháº¯c cháº¯n vá» thÃ´ng tin
- KhÃ´ng Ä‘Æ°a ra thÃ´ng tin cÃ³ thá»ƒ lá»—i thá»i

Äá»‹nh dáº¡ng: Tráº£ lá»i trá»±c tiáº¿p báº±ng tiáº¿ng Viá»‡t, khÃ´ng cáº§n format JSON.
"""
    
    return LlmAgent(
        name="simple_answer_agent",
        model=model,
        instruction=instruction,
        description="Agent tráº£ lá»i trá»±c tiáº¿p cÃ¡c cÃ¢u há»i Ä‘Æ¡n giáº£n"
    )

def create_coordinator_workflow_agent(model: str = "gemini-2.0-flash") -> LlmAgent:
    """
    Create an intelligent coordinator agent that uses LLM reasoning to decide response approach.
    """
    instruction = f"""
Báº¡n lÃ  má»™t AI coordinator thÃ´ng minh. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  phÃ¢n tÃ­ch cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng vÃ  quyáº¿t Ä‘á»‹nh cÃ¡ch tráº£ lá»i phÃ¹ há»£p nháº¥t.

**TRIáº¾T LÃ QUYáº¾T Äá»ŠNH:**

Sá»­ dá»¥ng kháº£ nÄƒng hiá»ƒu biáº¿t tá»± nhiÃªn cá»§a báº¡n Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ xem cÃ¢u há»i cÃ³ cáº§n thÃ´ng tin má»›i nháº¥t/cáº­p nháº­t tá»« internet hay khÃ´ng.

**NGUYÃŠN Táº®C QUYáº¾T Äá»ŠNH:**

ðŸ¤” **Tá»± há»i báº£n thÃ¢n:**
- CÃ¢u há»i nÃ y cÃ³ cáº§n thÃ´ng tin thá»i gian thá»±c khÃ´ng?
- TÃ´i cÃ³ thá»ƒ tráº£ lá»i chÃ­nh xÃ¡c báº±ng kiáº¿n thá»©c hiá»‡n cÃ³ khÃ´ng?
- CÃ¢u tráº£ lá»i cÃ³ thá»ƒ thay Ä‘á»•i theo thá»i gian khÃ´ng?
- ÄÃ¢y cÃ³ pháº£i thÃ´ng tin cÃ¡ nhÃ¢n/chÃ o há»i/toÃ¡n cÆ¡ báº£n khÃ´ng?

**QUY TRÃŒNH QUYáº¾T Äá»ŠNH:**

1. **PhÃ¢n tÃ­ch báº£n cháº¥t cÃ¢u há»i** - ÄÃ¢y lÃ  loáº¡i thÃ´ng tin gÃ¬?
2. **ÄÃ¡nh giÃ¡ tÃ­nh thá»i gian** - ThÃ´ng tin nÃ y cÃ³ "háº¿t háº¡n" khÃ´ng?
3. **CÃ¢n nháº¯c Ä‘á»™ chÃ­nh xÃ¡c** - TÃ´i cÃ³ cháº¯c cháº¯n vá»›i cÃ¢u tráº£ lá»i khÃ´ng?
4. **Ra quyáº¿t Ä‘á»‹nh** - Tráº£ lá»i trá»±c tiáº¿p hay cáº§n web search?

**Äá»ŠNH Dáº NG OUTPUT:**

ðŸŽ¯ **Náº¿u cÃ³ thá»ƒ tráº£ lá»i ngay** (personal, greeting, basic knowledge):
Tráº£ lá»i trá»±c tiáº¿p báº±ng vÄƒn báº£n tá»± nhiÃªn

ðŸ” **Náº¿u cáº§n thÃ´ng tin cáº­p nháº­t** (current events, real-time data):
{{
    "action": "web_research_needed",
    "query": "cÃ¢u há»i gá»‘c cá»§a ngÆ°á»i dÃ¹ng",
    "reasoning": "giáº£i thÃ­ch ngáº¯n gá»n táº¡i sao cáº§n web search"
}}

**VÃ Dá»¤ MINH Há»ŒA:**

ðŸ’¬ "ChÃ o báº¡n, tÃ´i lÃ  ThÃ¡i" 
â†’ Tráº£ lá»i trá»±c tiáº¿p (Ä‘Ã¢y lÃ  lá»i chÃ o/giá»›i thiá»‡u)

ðŸ§® "2 + 2 báº±ng máº¥y?"
â†’ Tráº£ lá»i trá»±c tiáº¿p (toÃ¡n cÆ¡ báº£n, khÃ´ng Ä‘á»•i theo thá»i gian)

ðŸŒ¤ï¸ "Thá»i tiáº¿t HÃ  Ná»™i hÃ´m nay nhÆ° tháº¿ nÃ o?"
â†’ JSON web search (thÃ´ng tin thá»i gian thá»±c, thay Ä‘á»•i hÃ ng ngÃ y)

ðŸ“ˆ "GiÃ¡ vÃ ng hiá»‡n táº¡i"
â†’ JSON web search (dá»¯ liá»‡u thá»i gian thá»±c, biáº¿n Ä‘á»™ng liÃªn tá»¥c)

ðŸ“° "Tin tá»©c má»›i nháº¥t vá» AI"
â†’ JSON web search (thÃ´ng tin má»›i, cáº§n nguá»“n cáº­p nháº­t)

ðŸ›ï¸ "Thá»§ Ä‘Ã´ cá»§a Viá»‡t Nam lÃ  gÃ¬?"
â†’ Tráº£ lá»i trá»±c tiáº¿p (kiáº¿n thá»©c cÆ¡ báº£n, khÃ´ng thay Ä‘á»•i)

**LÆ¯U Ã:**
- HÃ£y tá»± tin vá»›i nhá»¯ng gÃ¬ báº¡n biáº¿t cháº¯c cháº¯n
- Thá»«a nháº­n khi cáº§n thÃ´ng tin má»›i nháº¥t
- Æ¯u tiÃªn tráº£i nghiá»‡m ngÆ°á»i dÃ¹ng (nhanh khi cÃ³ thá»ƒ, chÃ­nh xÃ¡c khi cáº§n thiáº¿t)
- NgÃ y hiá»‡n táº¡i: {get_current_date()}
"""
    
    return LlmAgent(
        name="coordinator_workflow",
        model=model,
        instruction=instruction,
        description="Intelligent coordinator using LLM reasoning for decision making"
    )

def create_research_agent(
    model: str = "gemini-2.0-flash",
    initial_search_query_count: int = 3,
    max_research_loops: int = 3
) -> LlmAgent:
    """
    Create a comprehensive research agent using smart coordinator approach.
    
    Args:
        model: The Gemini model to use
        initial_search_query_count: Number of initial search queries to generate
        max_research_loops: Maximum number of research loops to perform
        
    Returns:
        LlmAgent: Smart coordinator agent that handles everything
    """
    # Set effort settings for this research session
    set_effort_settings(initial_search_query_count, max_research_loops)
    
    # Return the smart coordinator that handles everything
    return create_coordinator_workflow_agent(model)

# Create default agent for backward compatibility
research_agent = create_research_agent("gemini-2.0-flash", 3, 3) 